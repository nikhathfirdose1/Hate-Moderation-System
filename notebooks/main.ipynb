{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import dspy\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('../local.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ['GROQ_API_KEY'])\n",
    "GROQ_MODEL_LARGE = os.environ['GROQ_MODEL_LARGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_MODEL_LARGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = dspy.OllamaLocal(\n",
    "        model=\"llama3.2:3b\",\n",
    "        model_type='text',\n",
    "        max_tokens=1000,\n",
    "        temperature=0.1,\n",
    "        top_p=0.8,\n",
    "        frequency_penalty=1.17,\n",
    "        top_k=40\n",
    "    )\n",
    "dspy.configure(lm=llm, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextHateAnalysis(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Using the context and the comment, understand the whole meaning of the message and tell me if the comment conveys hate or not. \n",
    "    There are certain conditions to be considered before determining if the comment is hateful or not.\n",
    "    - The comment might or might not contain the hatefull words.\n",
    "    - If the comment contains the hatefull words, it might not be hateful. Its just the word used in a posive context and never conveyed hateful message.\n",
    "    - Even if the comment doesnt contain hatefull words in it, it doesnt mean it is not hateful. The comment might be hateful in a way that it is conveying with just non hateful words.\n",
    "    - Sarcasm and irony are also to be considered as hate.\n",
    "    - Classify as hateful if the comment is conveying hate even a little. Answer sensitively.\n",
    "    \n",
    "    Understand the whole context and determine the result.\n",
    "    \n",
    "    Output if the comment is Hateful or not. \"True\" if hateful, \"False\" if not hateful.\n",
    "    \"\"\"\n",
    "\n",
    "    comment: str = dspy.InputField()\n",
    "    context: str = dspy.InputField()\n",
    "    output = dspy.OutputField(\n",
    "        desc=\"\"\"Strictly Tell \"True\" if hateful, \"False\" if not hateful.\"\"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextHateExplaination(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Using the context and the comment, understand the whole meaning of the message and tell me if the comment conveys hate or not. \n",
    "    There are certain conditions to be considered before determining if the comment is hateful or not.\n",
    "    - The comment might or might not contain the hatefull words.\n",
    "    - If the comment contains the hatefull words, it might not be hateful. Its just the word used in a posive context and never conveyed hateful message.\n",
    "    - Even if the comment doesnt contain hatefull words in it, it doesnt mean it is not hateful. The comment might be hateful in a way that it is conveying with just non hateful words.\n",
    "    - Unserstand the sarcasm and irony in the comment. Sometimes the comment might be sarcastic and not hateful.\n",
    "    \n",
    "    Understand the whole context and determine the result. There might be simple terms which can still portray hate.\n",
    "    \n",
    "    Tell \"False\" if not hateful. \n",
    "    If hateful, provide the warning message in a format: \"The comment is hateful as it is targetting [specific person/group/community] in a [specific way].\"\n",
    "    \"\"\"\n",
    "\n",
    "    comment: str = dspy.InputField()\n",
    "    context: str = dspy.InputField()\n",
    "    output = dspy.OutputField(\n",
    "        desc=\"\"\"Strictly Tell \"False\" if not hateful. If hateful, provide the warning message.\"\"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dspy.ChainOfThought(ContextHateExplaination, max_retries=3).forward(\n",
    "    context=\"Common man to a celebrity\",\n",
    "    comment=\"I hate him\",\n",
    ")\n",
    "output = result.output\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dspy.ChainOfThought(ContextHateExplaination, max_retries=3).forward(\n",
    "    context=\"That was a great movie\",\n",
    "    comment=\"They killed it.\",\n",
    ")\n",
    "# output1 = result2.output\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "\n",
    "# def check_if_hateful(hateful_comment, context):\n",
    "#     prompt = f\"\"\"\n",
    "#    Analyze the following statement: '{hateful_comment}' in the context of '{context}'.\n",
    "\n",
    "# Determine if the comment conveys hate or not by following these steps:\n",
    "\n",
    "# 1. **Contextual Understanding**:\n",
    "#    - Fully understand the comment in its specific context, considering the surrounding situation, topic, and tone.\n",
    "#    - Analyze whether the comment could be hateful or not based on this context. Pay close attention to how different groups or individuals might interpret the comment (e.g., gender identity, sexual orientation, cultural background, religion, race/ethnicity/nationality, political ideologies, age groups, disability status, socioeconomic status/class divides, professional roles, or recreational interests like sports).\n",
    "\n",
    "# 2. **Abusive Language and Context Sensitivity**:\n",
    "#    - If the comment contains abusive or offensive words, assess whether these words are harmless/neutral or even positive within that specific context (e.g., casual sports banter or friendly teasing).\n",
    "#    - If the comment does not contain abusive or offensive words based on the {context} involves sensitive topics like gender identity, sexual orientation, race, politics, religion, or other personal identity issues, even comments without explicit hateful words can still convey hate through tone, implication, or stereotypes.\n",
    "#    - For example:\n",
    "#      - \"Don't behave like a girl\" directed at a man reinforces harmful gender stereotypes and could hurt LGBTQ+ individuals.\n",
    "#      - \"They killed it!\" in a sports game is a positive phrase despite containing the word \"kill.\"\n",
    "\n",
    "# 3. **Indirect Hate - Sarcasm/Irony**:\n",
    "#    - Check if the comment uses sarcasm, irony, or an indirect tone to convey hate. Even if the words themselves are not explicitly hateful, consider whether they carry a harmful subtext that could perpetuate stereotypes or marginalize a group.\n",
    "#    - For example:\n",
    "#      - \"You shouldn't be crying like this\" could be hateful if said as a jab toward someone overwhelmed at work but not hateful if said to console someone who is hurt.\n",
    "\n",
    "# 4. **Careful Analysis of Language**:\n",
    "#    - Recognize that comments without abusive language can still convey hate through tone and implication.\n",
    "#    - Similarly, comments with potentially harmful words may be harmless depending on context (e.g., slang in sports or entertainment).\n",
    "\n",
    "# 5. **Final Determination**:\n",
    "#    - After all broader and specific analysis steps are completed:\n",
    "#      - If the comment conveys hate (explicitly or implicitly), output \"True\" (hateful).\n",
    "#      - If it does not convey hate after careful consideration of all factors, output \"False\" (not hateful).\n",
    "\n",
    "# **Important Note**: Ensure sensitivity to how language interacts with various groups and contexts. Even seemingly neutral phrases can perpetuate harmful stereotypes or exclusion when directed at certain groups. Consider both explicit language and implicit subtext while making your determination.\"\"\"\n",
    "    \n",
    "#     response = ollama.chat(\n",
    "#         model=\"llama2:13b\",  \n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#     )\n",
    "    \n",
    "#     result = response[\"message\"][\"content\"]\n",
    "\n",
    "#     if \"True\" in result.lower():\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False \n",
    "\n",
    "\n",
    "# input_comment = \"They Killed It!\"\n",
    "# input_context = \"Told in a sports game\" \n",
    "\n",
    "# c1= \"Dont behave like a girl, be a man\"\n",
    "# t1 = \"Told to a guy\"\n",
    "\n",
    "# c2 = \"You shoudnt be crying like this\"\n",
    "# t2_hate = \"Jab towards a person who is overwhelemed at work\"\n",
    "# t2_nothate = \"Consoling someone cos they are hurt\"\n",
    "\n",
    "# c3= \"GO back Home!\"\n",
    "# t3= \"told to an immigrant worker\"\n",
    "\n",
    "# result = check_if_hateful(c3, t3)\n",
    "\n",
    "# if result is True:\n",
    "#     print(\"This comment is hateful.\")\n",
    "# else:\n",
    "#     print(\"This comment is not hateful in this context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def check_if_hateful(hateful_comment, context):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following comment: '{hateful_comment}' in the context: '{context}' for hate speech. Follow these steps carefully:\n",
    "\n",
    "    1. Understand Context:\n",
    "    Fully understand the {hateful_comment}'s intent within its {context}. Analyse whether it targets an individual or group based on identity, nationality, race, gender, or other protected characteristics.\n",
    "    Determine if the comment promotes exclusion, marginalization, or questions someone's right to belong within the given {context}.\n",
    "\n",
    "    2. Deeper Analysis:\n",
    "    Evaluate whether the language used carries implicit bias, dehumanizing tones, or indirect hostility, even if no explicit slurs are present.\n",
    "    Consider specific examples: phrases like \"go back home\" directed at an immigrant inherently promote exclusion and xenophobia and are considered hate speech.\n",
    "\n",
    "    3. Sarcasm Check:\n",
    "    Check if the tone is sarcastic, passive-aggressive, or coded to convey hate while appearing neutral. Pay attention to discriminatory undertones.\n",
    "    Assess whether the comment fosters a hostility for a particular group.\n",
    "\n",
    "    4. Results:\n",
    "    Output True if:\n",
    "    - The comment marginalizes or excludes based on identity, race, or other protected characteristics.\n",
    "    - It promotes hate, xenophobia, or hostility in the given context.\n",
    "    - The language implicitly or explicitly conveys discrimination or prejudice.\n",
    "\n",
    "    Output False if:\n",
    "    - The comment is neutral, positive, or contextually harmless.\n",
    "    - It does not promote exclusion, hostility, or discrimination.\n",
    "\n",
    "    Provide only \"True\" or \"False\" as the final output based on this analysis.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=\"llama2:13b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    result = response[\"message\"][\"content\"]\n",
    "\n",
    "    if \"true\" in result.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# input_comment = \"They Killed It!\"\n",
    "# input_context = \"Told in a sports game\"  \n",
    "\n",
    "# c1 = \"Dont behave like a girl, be a man\"\n",
    "# t1 = \"Told to a guy\"\n",
    "\n",
    "# c2 = \"You shoudnt be crying like this\"\n",
    "# t2_hate = \"Jab towards a person who is overwhelemed at work\"\n",
    "# t2_nothate = \"Consoling someone because they are hurt\"\n",
    "\n",
    "# c3 = \"GO back Home!\"\n",
    "# t3 = \"told to an immigrant worker\"\n",
    "\n",
    "# result = check_if_hateful(c3, t3)\n",
    "\n",
    "# if result is True:\n",
    "#     print(\"This comment is hateful.\")\n",
    "# else:\n",
    "#     print(\"This comment is not hateful in this context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_contextual_warning(hateful_comment, context):\n",
    "   \n",
    "    prompt = f\"\"\"  \n",
    "    Once the {hateful_comment} is classified as Hateful\n",
    "    Analyse why the Comment is Hateful:\n",
    "    Based on the input context, explain why the comment is considered hateful based on the given {context}. Identify if it uses explicit hate speech, implicit bias, sarcasm, or derogatory tones that target specific ideas, identities, or behaviors.\n",
    "    Affected Groups/Communities:\n",
    "    In the given context, analyze which groups or communities could be hurt by this comment. Consider factors such as:\n",
    "    The nature of the discussion (e.g., gender equality, race relations, political ideologies, LGBTQ community, Religion, ).\n",
    "    The groups explicitly or implicitly referenced in the comment.\n",
    "    The broader societal implications of such statements.\n",
    "    Potential Consequences:\n",
    "    Evaluate what could happen if such comments continue to be used. Include possible outcomes like:\n",
    "    Emotional harm or alienation of individuals/groups.\n",
    "    Public backlash or reputational damage for the user.\n",
    "    Escalation of tensions in sensitive discussions.\n",
    "    Contribution to systemic issues like discrimination or harassment.\n",
    "    Final Warning to User:\n",
    "    Provide a clear and sensitive two-sentence warning that highlights the harm caused by such language and advises against its use to prevent further consequences.\n",
    "    Give warning in this format: 'We understand that you may hold different beliefs and values, but the statement you are trying to make can hurt the sentiments of an individual based on the given context. Please avoid using such language to prevent misunderstandings or public backlash.'\n",
    "\n",
    "    Just output the warning to user starting from 'We undersatand.. format'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Give warning in this format: 'We understand that you may hold different beliefs and values, but this statement can hurt the sentiments of [inferred group/community]. Please avoid using such language to prevent misunderstandings or public backlash.'\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=\"llama2:13b\",  \n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# comment = \"Go back home!\"\n",
    "# context = \"Told to immigrants\"\n",
    "\n",
    "# warning_message = generate_contextual_warning(comment,context)\n",
    "\n",
    "# print(warning_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:   We understand that you may hold different beliefs and values, but the statement you are trying to make can hurt the sentiments of an individual based on the given context. Please avoid using such language to prevent misunderstandings or public backlash.\n"
     ]
    }
   ],
   "source": [
    "input_comment = \"They Killed It!\"\n",
    "input_context = \"Sports Game\"\n",
    "\n",
    "c1 = \"Dont behave like a girl, be a man\"\n",
    "t1 = \"Told to a guy\"\n",
    "\n",
    "c2 = \"You shoudnt be crying like this\"\n",
    "t2_hate = \"Jab towards a person who is overwhelemed at work\"\n",
    "t2_nothate = \"Consoling someone because they are hurt\"\n",
    "\n",
    "c3 = \"GO back Home!\"\n",
    "t3 = \"told to an immigrant worker\"\n",
    "\n",
    "result = check_if_hateful(c2, t2_nothate)\n",
    "\n",
    "if result: \n",
    "    warning_message = generate_contextual_warning(input_comment, input_context)\n",
    "    print(f\"Warning: {warning_message}\")\n",
    "else:  \n",
    "    print(\"This comment is not considered hateful in this context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
